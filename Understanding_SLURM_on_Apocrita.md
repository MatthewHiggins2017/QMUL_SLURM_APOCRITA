# Understanding SLURM on Apocrita

The goal of this document is to act as a guided tutorial and bring you up to speed using SLURM on Apocrita. 

---

# Precursors 

The following tutorial assumes you have:

1. Already gained access to the Apocrita HPC ([Request an account](https://slurm-docs.hpc.qmul.ac.uk/intro/hpc-account/))
2. Have fundamental understanding of how to use the command-line ([Linux introduction](https://slurm-docs.hpc.qmul.ac.uk/intro/linux_intro/))
3. Have set up SSH keys for login ([SSH keys guide](https://slurm-docs.hpc.qmul.ac.uk/intro/sshkeys/))

---

# What is Apocrita?

Apocrita is a heterogeneous HPC cluster comprising around **300 nodes** and **13,500 cores**, managed by ITS Research at Queen Mary University of London. It is located in the [ARK shared data centre](https://www.ark-d-c.com/locations/meridian-park) in Enfield, London.

## Why Use Apocrita?

Using an HPC cluster can substantially reduce the time taken to compute problems by:
- **Running analyses in parallel** ‚Äì multiple independent tasks run simultaneously
- **Breaking problems into pieces** ‚Äì allowing many separate compute cores (CPUs) to work on the same problem at the same time

The Apocrita system has been specifically designed to meet the needs of a wide variety of research across all three faculties: Humanities and Social Sciences, Science and Engineering, and Medicine and Dentistry.

---

# What is SLURM?

**SLURM** (Simple Linux Utility for Resource Management) is an open-source, highly scalable cluster management and job scheduling system. It is widely adopted across HPC sites worldwide and is under active development.

SLURM handles three key functions:

1. **Resource allocation** ‚Äì allocates exclusive and/or non-exclusive access to compute resources (nodes) to users for a duration of time
2. **Job scheduling** ‚Äì provides a framework for starting, executing, and monitoring work (typically batch jobs) on the set of allocated nodes
3. **Queue management** ‚Äì arbitrates contention for resources by managing a queue of pending jobs

### Key SLURM Commands

| Command | Description |
|---------|-------------|
| `sbatch job_script` | Submit a batch job script |
| `squeue --me` | Show your current jobs in the queue |
| `squeue` | Show all jobs in the queue |
| `scancel jobid` | Cancel a specific job |
| `scancel --me` | Cancel all your jobs |
| `salloc` | Request an interactive session |
| `sacct -j jobid` | View accounting info for a completed job |

---

# What Queues and Infrastructure are Available?

Apocrita runs a variety of different job types, and the cluster comprises different **node types** and **partitions** (queues) to accommodate this. Unless you have a specific technical reason, you should avoid selecting a specific node type to run your job on.

## Partitions Overview (DOUBLE CHECK THIS!)


| Partition | Description | Max Runtime |
|-----------|-------------|-------------|
| `compute` | Default partition for general jobs | 240 hours |
| `parallel` | For multi-node parallel (MPI) jobs | 240 hours |
| `highmem` | For high memory (>128hrs) CPU jobs | 240 hours |
| `gpushort` | Short GPU testing partition (free access) | 1 hour |
| `gpu` | Production GPU partition (requires approval) | 240 hours |

## Open-Access Node Types

| Node Type | GPU Flag | Count | Cores | RAM | Notes |
|-----------|----------|-------|-------|-----|-------|
| ddg (volta) | `--gres=gpu:X` | 2 | 48 | 384GB | GPU nodes |
| ddy | ‚Äì | 167 | 48 | 384GB | Standard compute |
| sbg (ampere) | `--gres=gpu:X` | 1 | 48 | 384GB | GPU nodes |
| sbg (volta) | `--gres=gpu:X` | 2 | 32 | 384GB | GPU nodes |
| sdx | ‚Äì | 52 | 36 | 384GB | Standard compute |
| smf | ‚Äì | 8 | 24 | 768GB | High memory |
| srm | ‚Äì | 2 | 36 | 768GB | High memory |
| vhm | ‚Äì | 4 | 64 | 3TB | Very high memory |

> **Note:** Hyperthreading is not enabled across the cluster. Some nodes are restricted depending on the source of funding.

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/nodes/

---

# What Do I Have Access to as a User?

## Home Directories

All users have a home directory mounted under `/data/home` (e.g., `/data/home/abc123`). 

| Feature | Details |
|---------|---------|
| **Location** | `/data/home/<username>` |
| **Size limit** | 100GB per user |
| **Accessibility** | Networked storage, accessible from all nodes |
| **Backups** | Backed up nightly |

‚ö†Ô∏è **Warning:** Avoid using your home directory as a destination for data generated by cluster jobs, as home directories fill up quickly and **exceeding your quota will cause your jobs to fail**.

---

# What is the Scratch Space?

Scratch space is used for **temporary storage** of files generated during cluster jobs. Jobs will likely run faster due to the higher performance of this system.

## Key Information

| Feature | Details |
|---------|---------|
| **Location** | `/gpfs/scratch/$USER` (e.g., `/gpfs/scratch/abc123`) |
| **Default quota** | 3TB capacity, 5 million files |
| **Auto-deletion** | Files deleted **65 days** after last modification |
| **Backup** | Snapshots kept for 1 week only |

## Important Notes

- If you have files not modified for 60 days, you will receive a **daily email** listing files due for deletion
- Check your current usage with the `qmquota` command
- **Do not abuse this service** by modifying timestamps to make it permanent storage ‚Äì you may lose access

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/storage/scratch/

---

# Using Modules

HPC clusters typically have many software packages installed, often with multiple versions. On Apocrita, we use the **modules** system to manage the user environment and avoid conflicts.

## Common Module Commands

| Command | Description |
|---------|-------------|
| `module avail` | List all available modules |
| `module avail <package>` | List versions of a specific package |
| `module load <package>` | Load a module into your environment |
| `module load <package>/<version>` | Load a specific version |
| `module list` | List currently loaded modules |
| `module unload <package>` | Unload a specific module |
| `module purge` | Unload all modules |

## Examples

```bash
# List all available MATLAB versions
module avail matlab

# Load the default version of MATLAB
module load matlab

# Load a specific version
module load matlab/2024a

# Check what's loaded
module list

# Unload everything
module purge
```

## Module Naming Conventions

Modules follow the `NAME/VERSION` convention. Spack-generated modules also include the compiler:

- `app/1.1.0-gcc-12.2.0` ‚Äì version 1.1.0 compiled with GCC 12.2
- `app/1.1.0-openmpi-5.0.3-gcc-12.2.0` ‚Äì includes OpenMPI 5.0.3
- `app/1.1.0-openmpi-5.0.3-cuda-12.4.0-gcc-12.2.0` ‚Äì includes CUDA 12.4

## Dependencies

Some modules have dependencies that are **automatically loaded**:

```bash
$ module load hdf5
Loading hdf5/1.14.3-openmpi-5.0.3-gcc-12.2.0
  Loading requirement: openmpi/5.0.3-gcc-12.2.0
```

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/using/UsingModules/

---

# Creating Your First Job Script

A SLURM job script is a shell script with special `#SBATCH` directives that tell SLURM what resources your job needs.

## Job Script Header

All `#SBATCH` lines **must come before** any ordinary commands in your script. Any `#SBATCH` lines appearing after the first non-`#SBATCH` line will be ignored.

‚ö†Ô∏è **Important:** Use `#SBATCH` (with the letter "S"), not `#$BATCH` (with a dollar symbol).

## Useful Environment Variables

| Variable | Description |
|----------|-------------|
| `$SLURM_JOB_ID` | Unique job identifier |
| `$SLURM_JOB_NAME` | Name of the job |
| `$SLURM_NTASKS` | Number of tasks/cores requested |
| `$SLURM_SUBMIT_DIR` | Directory where job was submitted |
| `$SLURM_ARRAY_TASK_ID` | Task ID in an array job |

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/using/uge_to_slurm/


## Common SBATCH Options

| Option | Description | Example |
|--------|-------------|---------|
| `-n` or `--ntasks` | Number of CPU cores | `-n 4` |
| `-t` | Maximum runtime (HH:MM:SS) | `-t 1:0:0` (1 hour) |
| `--mem-per-cpu` | RAM per core | `--mem-per-cpu=4G` |
| `-p` or `--partition` | Queue/partition | `-p compute` |
| `-J` or `--job-name` | Name for the job | `-J my_analysis` |
| `-o` | Output file pattern | `-o %x.o%j` |
| `-e` | Error file pattern | `-e %x.e%j` |
| `--mail-type` | Email notifications | `--mail-type=ALL` |
| `--mail-user` | Email address | `--mail-user=abc123@qmul.ac.uk` |

## Example: Simple Single-Core Job

Here's a minimal job script that runs for about 1 minute:

```bash
#!/bin/bash
#SBATCH -J my_first_job       # Job name
#SBATCH -o %x.o%j             # Output file: jobname.o<jobid>
#SBATCH -e %x.e%j             # Error file: jobname.e<jobid>
#SBATCH -n 1                  # Request 1 core
#SBATCH -t 0:5:0              # Request 5 minutes runtime
#SBATCH --mem-per-cpu=1G      # Request 1GB RAM

# Print some information
echo "Job started at: $(date)"
echo "Running on node: $(hostname)"
echo "Job ID: ${SLURM_JOB_ID}"

# Simulate some work (wait for 60 seconds)
echo "Sleeping for 60 seconds..."
sleep 60

echo "Job completed at: $(date)"
```

## Example: Multi-Core Job

For applications that can use multiple threads:

```bash
#!/bin/bash
#SBATCH -J multi_core_job     # Job name
#SBATCH -o %x.o%j             # Output file
#SBATCH -n 8                  # Request 8 cores
#SBATCH -t 2:0:0              # Request 2 hours runtime
#SBATCH --mem-per-cpu=4G      # 4GB per core = 32GB total

# Load required modules
module load samtools

# Run application using $SLURM_NTASKS for thread count
samtools sort -@ ${SLURM_NTASKS} input.bam -o output.sorted.bam

echo "Job completed successfully"
```

---

# Submitting Your Script

## The `sbatch` Command

Submit your job script using `sbatch`:

```bash
sbatch my_job_script.sh
```

Upon successful submission, you'll see:
```
Submitted batch job 1234567
```

## Checking Job Status

```bash
# View your jobs
squeue --me

# View all jobs (can be very long!)
squeue

# View detailed job information
scontrol show job 1234567
```

## Understanding Output Files

By default, SLURM creates a single output file called `slurm-<JOBID>.out` containing both STDOUT and STDERR.

### Custom Output Naming

Using the `%x` (job name) and `%j` (job ID) wildcards:

```bash
#SBATCH -J my_analysis
#SBATCH -o %x.o%j    # Creates: my_analysis.o1234567
#SBATCH -e %x.e%j    # Creates: my_analysis.e1234567
```

### Output File Contents

| File | Contains |
|------|----------|
| `.o` file (STDOUT) | Normal output from your program |
| `.e` file (STDERR) | Error messages and warnings |

## Selecting Appropriate Resources

### Memory Requests

- `--mem-per-cpu` must be an integer (e.g., use `--mem-per-cpu=7500M` not `--mem-per-cpu=7.5G`)
- Request only what you need ‚Äì over-requesting wastes resources and increases queue time

### Runtime Estimates

- Estimate generously but reasonably
- Jobs exceeding their time limit are **killed immediately**
- Use `sacct -j <jobid>` to check how long previous jobs took

---

# Array Jobs

Array jobs allow you to submit many similar jobs with a single script ‚Äì extremely useful in bioinformatics for processing multiple samples.

## Why Use Array Jobs in Bioinformatics?

- **Process multiple FASTQ files** through the same pipeline
- **Run the same analysis** with different parameters
- **Map reads** from multiple samples to a reference genome
- **Perform variant calling** on many BAM files

## Basic Array Job Structure

```bash
#!/bin/bash
#SBATCH -J my_array_job
#SBATCH -o logs/%x.o%A.%a     # %A = array job ID, %a = task ID
#SBATCH -n 1
#SBATCH --mem-per-cpu=4G
#SBATCH -t 2:0:0
#SBATCH -a 1-10               # Run 10 tasks (IDs 1-10)

echo "This is task ${SLURM_ARRAY_TASK_ID} of array job ${SLURM_ARRAY_JOB_ID}"
```

## Array Options

| Option | Description | Example |
|--------|-------------|---------|
| `-a 1-100` | Run tasks 1 to 100 | 100 tasks |
| `-a 0-99` | Run tasks 0 to 99 | 100 tasks (0-indexed) |
| `-a 1-100:5` | Step size of 5 | Tasks: 1, 6, 11, 16... |
| `-a 1-100%10` | Max 10 concurrent tasks | Limits parallel execution |

## Example: Processing Multiple FASTQ Files with minimap2

### Step 1: Create a list of input files

```bash
ls -1 /path/to/fastq/*.fastq.gz > sample_list.txt
wc -l sample_list.txt  # Check number of files
```

### Step 2: Create the array job script

```bash
#!/bin/bash
#SBATCH -J minimap2_array
#SBATCH -o logs/%x.o%A.%a
#SBATCH -e logs/%x.e%A.%a
#SBATCH -n 8
#SBATCH --mem-per-cpu=4G
#SBATCH -t 4:0:0
#SBATCH -a 1-50%10            # 50 samples, max 10 running at once

# Load required module
module load minimap2

# Reference genome
REFERENCE="/path/to/reference.fasta"

# Get the input file for this task
INPUT_FILE=$(sed -n "${SLURM_ARRAY_TASK_ID}p" sample_list.txt)

# Extract sample name (without path and extension)
SAMPLE=$(basename ${INPUT_FILE} .fastq.gz)

# Define output
OUTPUT="/gpfs/scratch/${USER}/alignments/${SAMPLE}.sam"

# Run minimap2
echo "Processing ${SAMPLE}..."
minimap2 -t ${SLURM_NTASKS} -ax map-ont ${REFERENCE} ${INPUT_FILE} > ${OUTPUT}

echo "Completed ${SAMPLE}"
```

## Managing Array Jobs

```bash
# Cancel specific tasks (e.g., tasks 20-60 of job 3388)
scancel 3388_[20-60]

# Hold specific tasks
scontrol hold 3388_20-60

# Release held tasks
scontrol release 3388_20-60

# Resubmit failed tasks only
sbatch -a 5,17,35 my_array_script.sh
```

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/using/arrays/

---

# GPU Jobs

Apocrita contains GPU nodes with NVIDIA cards (Volta, Ampere, Hopper) for accelerated computing.

## Access Requirements

| Partition | Access | Max Runtime | Purpose |
|-----------|--------|-------------|---------|
| `gpushort` | Open to all QMUL researchers | 1 hour | Testing only |
| `gpu` | Requires approval | 240 hours | Production jobs |

‚ö†Ô∏è **Note:** GPU access is not permitted for Undergraduate/MSc students or external users.

## Requesting GPUs

Use the `--gres=gpu:<count>` option:

```bash
#SBATCH -p gpushort          # GPU partition
#SBATCH -n 8                 # 8 cores (8 per GPU recommended)
#SBATCH --cpus-per-gpu=8     # Cores per GPU
#SBATCH --mem-per-cpu=11G    # RAM per core
#SBATCH --gres=gpu:1         # Request 1 GPU
```

## Example: GPU Job for Nanopore Basecalling with Dorado

```bash
#!/bin/bash
#SBATCH -J dorado_basecall
#SBATCH -o %x.o%j
#SBATCH -e %x.e%j
#SBATCH -p gpushort           # Use gpushort for testing
#SBATCH -n 8
#SBATCH --cpus-per-gpu=8
#SBATCH --mem-per-cpu=11G
#SBATCH -t 1:0:0              # 1 hour max for gpushort
#SBATCH --gres=gpu:1          # Request 1 GPU

# Load modules
module load dorado

# Define paths
POD5_DIR="/gpfs/scratch/${USER}/pod5_files"
OUTPUT_DIR="/gpfs/scratch/${USER}/basecalled"
MODEL="dna_r10.4.1_e8.2_400bps_sup@v4.2.0"

# Create output directory
mkdir -p ${OUTPUT_DIR}

# Run basecalling
dorado basecaller ${MODEL} ${POD5_DIR} > ${OUTPUT_DIR}/calls.bam

echo "Basecalling completed"
```

## Selecting Specific GPU Types

```bash
# Request only Ampere A100 GPUs
#SBATCH --constraint=ampere

# Request only Hopper H100/H200 GPUs
#SBATCH --constraint=hopper
```

## Checking GPU Usage

SSH into your allocated node and run:

```bash
# Basic GPU monitoring
nvidia-smi -l 1

# Advanced monitoring (load module first)
module load nvtools
nvtop    # or nvitop
```

## Multiple GPUs (Whole Node)

```bash
#!/bin/bash
#SBATCH -J multi_gpu_job
#SBATCH -p gpu
#SBATCH -n 32                 # All cores
#SBATCH --cpus-per-gpu=8
#SBATCH -t 48:0:0
#SBATCH --mem=0               # All RAM
#SBATCH --gres=gpu:4          # All 4 GPUs
#SBATCH --exclusive           # Exclusive node access
```

‚ö†Ô∏è **Important:** When requesting `--exclusive`, you must also request `--mem=0` to get all available RAM.

üìñ **Documentation:** https://slurm-docs.hpc.qmul.ac.uk/using/usingGPU/

---

# Quick Reference Card

## Essential Commands

```bash
# Submit a job
sbatch script.sh

# Check your jobs
squeue --me

# Cancel a job
scancel <jobid>

# Check job details
sacct -j <jobid>

# Interactive session
salloc
```

## Minimal Job Script Template

```bash
#!/bin/bash
#SBATCH -J jobname
#SBATCH -o %x.o%j
#SBATCH -n 1
#SBATCH --mem-per-cpu=4G
#SBATCH -t 1:0:0

module load <your_software>

# Your commands here
```

---

# Getting Help

- **Email:** its-research-support@qmul.ac.uk
- **Documentation:** https://slurm-docs.hpc.qmul.ac.uk
- **Quick Reference:** https://slurm-docs.hpc.qmul.ac.uk/using/quick_guide/
- **FAQs:** https://slurm-docs.hpc.qmul.ac.uk/faqs/
